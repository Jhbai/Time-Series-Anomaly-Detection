{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ea80d9b-2f1b-4e40-9ed2-4f8003cda360",
   "metadata": {},
   "source": [
    "# Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a1d6a35-3b03-4b9d-a3a9-f89f5f218faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型架構套件\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 資料處理套件\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 最佳化到件\n",
    "import torch.optim as optim\n",
    "\n",
    "# others常見套件\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a1e540-36e9-4db6-a868-d65ac3c5b3a8",
   "metadata": {},
   "source": [
    "# Data Processing Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fbd8b29-535a-404a-a3a8-d011bbbc58db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定隨機種子以確保可重現性\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# 定義數據集\n",
    "class PressurePumpDataset(Dataset):\n",
    "    def __init__(self, num_samples=10000, anomaly_ratio=0.05):\n",
    "        self.num_samples = num_samples\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        for _ in range(num_samples):\n",
    "            A = float(np.random.normal(loc=5.0, scale=1.5))\n",
    "            B = float(np.random.binomial(1, 0.5))\n",
    "            \n",
    "            if A > 8.0:\n",
    "                label = 1 if B == 1 else 0\n",
    "            else:\n",
    "                label = 1\n",
    "                \n",
    "            if label == 0 and random.random() > anomaly_ratio:\n",
    "                label = 1\n",
    "                \n",
    "            self.data.append([A, B])\n",
    "            self.labels.append(label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f5ad186-59d2-48d9-910b-72f0ae27ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PressurePumpDataset(num_samples=8000, anomaly_ratio=0.1)\n",
    "test_dataset = PressurePumpDataset(num_samples=2000, anomaly_ratio=0.1)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45a7d74-4cf9-4946-88e9-8b5180c3d3e4",
   "metadata": {},
   "source": [
    "# Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "095dfdef-6543-439e-bc58-c74ec1cf9bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        output_dim = input_dim\n",
    "        \n",
    "        # key, query, value\n",
    "        self.weights = nn.ParameterList(\n",
    "            [nn.Parameter(torch.randn(input_dim, output_dim)) for _ in range(3)]\n",
    "        )\n",
    "        self.biases = nn.ParameterList(\n",
    "            [nn.Parameter(torch.zeros(output_dim)) for _ in range(3)]\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        k = x1@(self.weights[0]) + self.biases[0]\n",
    "        v = x1@(self.weights[1]) + self.biases[1]\n",
    "        q = x2@(self.weights[2]) + self.biases[2]\n",
    "        return (q@(k.T)/(k.shape[-1])**0.5)@(v)\n",
    "\n",
    "class InteractionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(InteractionLayer, self).__init__()\n",
    "        self.interaction = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Residual Connection\n",
    "        return self.interaction(x) + x\n",
    "\n",
    "class JointAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(JointAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder A: Dealing with Continuous Data\n",
    "        self.encoder_A = nn.Sequential(nn.Linear(1, 32), nn.ReLU(), nn.Linear(32, 16), nn.ReLU())\n",
    "        \n",
    "        # Encoder B: Dealing with Descrete Data\n",
    "        # Its a status code with 1 bit.\n",
    "        self.encoder_B = nn.Sequential(nn.Embedding(num_embeddings=2, embedding_dim=4), nn.Linear(4, 16), nn.ReLU())\n",
    "        \n",
    "        # Ineraction Layer\n",
    "        self.interaction = InteractionLayer(input_dim=32, hidden_dim=16)\n",
    "        \n",
    "        # Attention Layer\n",
    "        self.attention = AttentionLayer(input_dim=16)\n",
    "        \n",
    "        # Latent Space\n",
    "        self.latent = nn.Sequential(nn.Linear(16, 8), nn.Tanh())\n",
    "        \n",
    "        # Decoder A\n",
    "        self.decoder_A = nn.Sequential(nn.Linear(8, 16), nn.ReLU(), nn.Linear(16, 1))\n",
    "        \n",
    "        # Decoder B\n",
    "        self.decoder_B = nn.Sequential(nn.Linear(8, 8), nn.ReLU(), nn.Linear(8, 2), nn.ReLU(), nn.Linear(2, 1), nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, A, B):\n",
    "        x_A = self.encoder_A(A)  # [batch_size, 16]\n",
    "        x_B = self.encoder_B(B.long()).squeeze(1)  # [batch_size, 16]\n",
    "        attended = self.attention(x_A, x_B)  # [batch_size, 32]\n",
    "        z = self.latent(attended)  # [batch_size, 16]\n",
    "        decoded_A = self.decoder_A(z)  # [batch_size, 1]\n",
    "        decoded_B = self.decoder_B(z)  # [batch_size, 2]\n",
    "        return decoded_A, decoded_B, z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac62af7b-4a1b-4bed-8e19-723ec9f6c4f5",
   "metadata": {},
   "source": [
    "# Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94ed73a9-4b20-482e-8f7c-8dce8e618462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練函數\n",
    "def train_autoencoder(model, loader, criterion_A, criterion_B, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_data, _ in loader:\n",
    "        A = batch_data[:, 0].unsqueeze(1).to(device)  # [batch_size, 1]\n",
    "        B = batch_data[:, 1].unsqueeze(1).to(device)  # [batch_size, 1]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        recon_A, recon_B, _ = model(A, B)\n",
    "        \n",
    "        # 重建損失\n",
    "        loss_A = criterion_A(recon_A, A)\n",
    "        loss_B = criterion_B(recon_B.squeeze(1), B.squeeze(1))\n",
    "        loss = loss_A + loss_B\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * A.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "# 評估函數\n",
    "def evaluate_autoencoder(model, loader, criterion_A, criterion_B, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_data, _ in loader:\n",
    "            A = batch_data[:, 0].unsqueeze(1).to(device)\n",
    "            B = batch_data[:, 1].unsqueeze(1).to(device)\n",
    "            \n",
    "            recon_A, recon_B, _ = model(A, B)\n",
    "            \n",
    "            loss_A = criterion_A(recon_A, A)\n",
    "            loss_B = criterion_B(recon_B.squeeze(1), B.squeeze(1))\n",
    "            loss = loss_A + loss_B\n",
    "            \n",
    "            running_loss += loss.item() * A.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1342ce74-7c01-4e31-8f74-6ca01b967482",
   "metadata": {},
   "source": [
    "# Training Object Assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b4c27e7-0fe2-4c70-b751-e5d30e87b447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 設定設備\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# 初始化模型、損失函數和優化器\n",
    "model = JointAutoencoder().to(device)\n",
    "criterion_A = nn.MSELoss().to(device)\n",
    "criterion_B = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# 訓練過程\n",
    "num_epochs = 1000\n",
    "best_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47664ae9-d5dc-4928-8bb3-040e4bfb5979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 | Train Loss: 152.4264 | Val Loss: 147.4524\n",
      "Epoch 5/1000 | Train Loss: 133.3960 | Val Loss: 138.4540\n",
      "Epoch 10/1000 | Train Loss: 133.3963 | Val Loss: 138.4541\n",
      "Epoch 15/1000 | Train Loss: 133.3962 | Val Loss: 138.4539\n",
      "Epoch 20/1000 | Train Loss: 133.3963 | Val Loss: 138.4542\n",
      "Epoch 25/1000 | Train Loss: 133.3967 | Val Loss: 138.4565\n",
      "Epoch 30/1000 | Train Loss: 133.3990 | Val Loss: 138.4547\n",
      "Epoch 35/1000 | Train Loss: 133.3969 | Val Loss: 138.4613\n",
      "Epoch 40/1000 | Train Loss: 133.3980 | Val Loss: 138.4594\n",
      "Epoch 45/1000 | Train Loss: 133.3984 | Val Loss: 138.4539\n",
      "Epoch 50/1000 | Train Loss: 133.4005 | Val Loss: 138.4539\n",
      "Epoch 55/1000 | Train Loss: 133.4006 | Val Loss: 138.4578\n",
      "Epoch 60/1000 | Train Loss: 133.3976 | Val Loss: 138.4546\n",
      "Epoch 65/1000 | Train Loss: 133.4010 | Val Loss: 138.4550\n",
      "Epoch 70/1000 | Train Loss: 133.3982 | Val Loss: 138.4540\n",
      "Epoch 75/1000 | Train Loss: 133.3990 | Val Loss: 138.4564\n",
      "Epoch 80/1000 | Train Loss: 133.3978 | Val Loss: 138.4542\n",
      "Epoch 85/1000 | Train Loss: 133.3982 | Val Loss: 138.4594\n",
      "Epoch 90/1000 | Train Loss: 133.3980 | Val Loss: 138.4539\n",
      "Epoch 95/1000 | Train Loss: 133.3997 | Val Loss: 138.4600\n",
      "Epoch 100/1000 | Train Loss: 133.3953 | Val Loss: 138.4544\n",
      "Epoch 105/1000 | Train Loss: 133.3989 | Val Loss: 138.4573\n",
      "Epoch 110/1000 | Train Loss: 133.3987 | Val Loss: 138.4541\n",
      "Epoch 115/1000 | Train Loss: 133.3977 | Val Loss: 138.4605\n",
      "Epoch 120/1000 | Train Loss: 133.3985 | Val Loss: 138.4551\n",
      "Epoch 125/1000 | Train Loss: 133.3980 | Val Loss: 138.4539\n",
      "Epoch 130/1000 | Train Loss: 133.4004 | Val Loss: 138.4539\n",
      "Epoch 135/1000 | Train Loss: 133.4006 | Val Loss: 138.4640\n",
      "Epoch 140/1000 | Train Loss: 133.3988 | Val Loss: 138.4546\n",
      "Epoch 145/1000 | Train Loss: 133.3983 | Val Loss: 138.4561\n",
      "Epoch 150/1000 | Train Loss: 133.3976 | Val Loss: 138.4540\n",
      "Epoch 155/1000 | Train Loss: 133.3981 | Val Loss: 138.4550\n",
      "Epoch 160/1000 | Train Loss: 133.4004 | Val Loss: 138.4555\n",
      "Epoch 165/1000 | Train Loss: 133.3980 | Val Loss: 138.4562\n",
      "Epoch 170/1000 | Train Loss: 133.3972 | Val Loss: 138.4580\n",
      "Epoch 175/1000 | Train Loss: 133.3982 | Val Loss: 138.4553\n",
      "Epoch 180/1000 | Train Loss: 133.3999 | Val Loss: 138.4540\n",
      "Epoch 185/1000 | Train Loss: 133.3989 | Val Loss: 138.4542\n",
      "Epoch 190/1000 | Train Loss: 133.3989 | Val Loss: 138.4585\n",
      "Epoch 195/1000 | Train Loss: 133.4020 | Val Loss: 138.4542\n",
      "Epoch 200/1000 | Train Loss: 133.3981 | Val Loss: 138.4544\n",
      "Epoch 205/1000 | Train Loss: 133.4012 | Val Loss: 138.4539\n",
      "Epoch 210/1000 | Train Loss: 133.4001 | Val Loss: 138.4556\n",
      "Epoch 215/1000 | Train Loss: 133.3989 | Val Loss: 138.4541\n",
      "Epoch 220/1000 | Train Loss: 133.4005 | Val Loss: 138.4544\n",
      "Epoch 225/1000 | Train Loss: 133.3978 | Val Loss: 138.4544\n",
      "Epoch 230/1000 | Train Loss: 133.3973 | Val Loss: 138.4564\n",
      "Epoch 235/1000 | Train Loss: 133.3958 | Val Loss: 138.4540\n",
      "Epoch 240/1000 | Train Loss: 133.3983 | Val Loss: 138.4553\n",
      "Epoch 245/1000 | Train Loss: 133.3974 | Val Loss: 138.4545\n",
      "Epoch 250/1000 | Train Loss: 133.3983 | Val Loss: 138.4571\n",
      "Epoch 255/1000 | Train Loss: 133.3991 | Val Loss: 138.4541\n",
      "Epoch 260/1000 | Train Loss: 133.3963 | Val Loss: 138.4563\n",
      "Epoch 265/1000 | Train Loss: 133.3985 | Val Loss: 138.4550\n",
      "Epoch 270/1000 | Train Loss: 133.3978 | Val Loss: 138.4712\n",
      "Epoch 275/1000 | Train Loss: 133.3979 | Val Loss: 138.4542\n",
      "Epoch 280/1000 | Train Loss: 133.4005 | Val Loss: 138.4587\n",
      "Epoch 285/1000 | Train Loss: 133.3999 | Val Loss: 138.4541\n",
      "Epoch 290/1000 | Train Loss: 133.3973 | Val Loss: 138.4543\n",
      "Epoch 295/1000 | Train Loss: 133.3980 | Val Loss: 138.4581\n",
      "Epoch 300/1000 | Train Loss: 133.3973 | Val Loss: 138.4540\n",
      "Epoch 305/1000 | Train Loss: 133.3991 | Val Loss: 138.4554\n",
      "Epoch 310/1000 | Train Loss: 133.3968 | Val Loss: 138.4543\n",
      "Epoch 315/1000 | Train Loss: 133.3989 | Val Loss: 138.4540\n",
      "Epoch 320/1000 | Train Loss: 133.3991 | Val Loss: 138.4619\n",
      "Epoch 325/1000 | Train Loss: 133.3965 | Val Loss: 138.4540\n",
      "Epoch 330/1000 | Train Loss: 133.3966 | Val Loss: 138.4540\n",
      "Epoch 335/1000 | Train Loss: 133.3966 | Val Loss: 138.4567\n",
      "Epoch 340/1000 | Train Loss: 133.3973 | Val Loss: 138.4627\n",
      "Epoch 345/1000 | Train Loss: 133.3976 | Val Loss: 138.4549\n",
      "Epoch 350/1000 | Train Loss: 133.4010 | Val Loss: 138.4658\n",
      "Epoch 355/1000 | Train Loss: 133.3983 | Val Loss: 138.4626\n",
      "Epoch 360/1000 | Train Loss: 133.3988 | Val Loss: 138.4559\n",
      "Epoch 365/1000 | Train Loss: 133.3988 | Val Loss: 138.4545\n",
      "Epoch 370/1000 | Train Loss: 133.3965 | Val Loss: 138.4631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_autoencoder(model, train_loader, criterion_A, criterion_B, optimizer, device)\n",
    "    val_loss = evaluate_autoencoder(model, test_loader, criterion_A, criterion_B, device)\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_autoencoder.pth')\n",
    "    \n",
    "    if (epoch+1) % 5 == 0 or epoch == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "\n",
    "# 異常檢測\n",
    "# 加載最佳模型\n",
    "model.load_state_dict(torch.load('best_autoencoder.pth'))\n",
    "model.eval()\n",
    "\n",
    "# 定義計算重建誤差的函數\n",
    "def calculate_reconstruction_error(model, loader, device):\n",
    "    errors = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in loader:\n",
    "            A = batch_data[:, 0].unsqueeze(1).to(device)\n",
    "            B = batch_data[:, 1].unsqueeze(1).to(device)\n",
    "            recon_A, recon_B, _ = model(A, B)\n",
    "            \n",
    "            # 計算重建誤差\n",
    "            loss_A = F.mse_loss(recon_A, A, reduction='none')\n",
    "            loss_A = loss_A.mean(dim=1)  # [batch_size]\n",
    "            \n",
    "            # 對於重建B的部分，使用交叉熵損失\n",
    "            loss_B = F.cross_entropy(recon_B.squeeze(1), B.squeeze(1), reduction='none')  # [batch_size]\n",
    "            \n",
    "            # 總重建誤差\n",
    "            loss = loss_A + loss_B  # [batch_size]\n",
    "            \n",
    "            errors.extend(loss.cpu().numpy())\n",
    "            labels.extend(batch_labels.numpy())\n",
    "    return np.array(errors), np.array(labels)\n",
    "\n",
    "# 計算測試集的重建誤差\n",
    "errors, labels = calculate_reconstruction_error(model, test_loader, device)\n",
    "\n",
    "# 繪製重建誤差分佈\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(errors[labels==1], color='green', label='Normal', kde=True, stat=\"density\", bins=50)\n",
    "sns.histplot(errors[labels==0], color='red', label='Anomaly', kde=True, stat=\"density\", bins=50)\n",
    "plt.legend()\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Reconstruction Error Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce02175-dd49-417b-8930-4c3f273af8a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b42d84c-157d-460b-ada1-55ddddd004b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhbai\\AppData\\Local\\Temp\\ipykernel_34552\\2847560561.py:25: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  sequences = torch.tensor(sequences, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/1000], Loss: 1.0323\n",
      "Epoch [400/1000], Loss: 0.7619\n",
      "Epoch [600/1000], Loss: 0.6418\n",
      "Epoch [800/1000], Loss: 0.5482\n",
      "Epoch [1000/1000], Loss: 0.4814\n",
      "每個序列的重建誤差（MSE）: [0.44693908 0.4459344  0.5328663  0.4982229  0.5576994 ]\n",
      "異常序列為: E\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# 設定隨機種子以便重現結果\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# 假設每個序列的長度為 100\n",
    "sequence_length = 100\n",
    "num_sequences = 5\n",
    "\n",
    "# 生成五個隨機氣壓時間序列（A、B、C、D、E）\n",
    "# 假設正常序列來自正態分佈，異常序列有較大偏移\n",
    "sequences = []\n",
    "for i in range(num_sequences):\n",
    "    if i == 4:  # 假設第五個序列是異常的\n",
    "        seq = np.random.normal(loc=10.0, scale=1.0, size=sequence_length)\n",
    "    else:\n",
    "        seq = np.random.normal(loc=0.0, scale=1.0, size=sequence_length)\n",
    "    sequences.append(seq)\n",
    "\n",
    "# 將數據轉換為 PyTorch 張量，並進行標準化\n",
    "sequences = torch.tensor(sequences, dtype=torch.float32)\n",
    "# 標準化每個序列\n",
    "sequences = (sequences - sequences.mean(dim=1, keepdim=True)) / sequences.std(dim=1, keepdim=True)\n",
    "\n",
    "# 添加一個維度以符合 LSTM 的輸入要求 (batch, seq, feature)\n",
    "sequences = sequences.unsqueeze(-1)  # shape: (5, 100, 1)\n",
    "\n",
    "class LSTM_Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2):\n",
    "        super(LSTM_Autoencoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # 編碼器\n",
    "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # 解碼器\n",
    "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 編碼\n",
    "        encoded, (hidden, cell) = self.encoder(x)\n",
    "        \n",
    "        # 解碼\n",
    "        decoded, _ = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# 選擇正常的序列進行訓練（假設前四個是正常的）\n",
    "train_data = sequences[:4]  # shape: (4, 100, 1)\n",
    "\n",
    "# 定義模型、損失函數和優化器\n",
    "model = LSTM_Autoencoder(input_size=1, hidden_size=64, num_layers=2)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 訓練參數\n",
    "num_epochs = 1000\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(train_data)\n",
    "    loss = criterion(outputs, train_data)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 200 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 評估並檢測異常\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructed = model(sequences)\n",
    "    # 計算每個序列的均方誤差\n",
    "    mse = torch.mean((reconstructed - sequences) ** 2, dim=(1,2))\n",
    "    print(\"每個序列的重建誤差（MSE）:\", mse.numpy())\n",
    "\n",
    "    # 假設重建誤差最大的序列為異常\n",
    "    anomaly_index = torch.argmax(mse).item()\n",
    "    sequence_names = ['A', 'B', 'C', 'D', 'E']\n",
    "    print(f\"異常序列為: {sequence_names[anomaly_index]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06fdd5e-7025-487d-9114-1adfde11723c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
